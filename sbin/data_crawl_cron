#!/evaluation_system_tmp/mypython/bin/python
# encoding: utf-8

import os
from evaluation_system.commands import BaseCommand
from evaluation_system.model.solr_models.models import UserCrawl
import subprocess

class CrawlPrjectdataCronjob(BaseCommand):
    _short_args = 'hd'
    _args = ['debug', 'help']
    __short_description__ = '''\nRun this file as Cronjob to crawl projectdata.'''

    def _run(self):
      
        path = os.path.dirname(os.path.abspath(__file__))
        script_path = path + '/crawl_miklip'
        for  crawl_item in UserCrawl.objects.filter(status='waiting'):
            crawl_item.status = 'crawling'
            crawl_item.save()
            out = 'Starting crawl for path %s\n' % crawl_item.path_to_crawl
	    out += subprocess.Popen('/bin/bash '+script_path + ' ' + crawl_item.path_to_crawl, shell=True, stdout=subprocess.PIPE).stdout.read()
         
            #get dumpfile name
	    for line in out.split('\n'):
                if 'solr_crawl_' in line:
                    df_name = line.split('/')[-1]
		    print df_name
	    out += '\nDone crawl for path\nCrawl is sent to server\n \n'
            crawl_item.tar_file = df_name
            crawl_item.ingest_msg = out
	    crawl_item.save()
if __name__ == "__main__":
    inst = CrawlPrjectdataCronjob().run()
    #inst.run()


